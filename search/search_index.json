{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Synthetic Clinical Data Benchmarking \u00b6 This microsite describes the generation of synthetic clinical data and the loading of this data into a distributed graph database such as TigerGraph for benchmarking purposes. The process of generating large collections of clinical graph data can be used for: Establishing the baseline performance for bringing an entire clinical record for a patent when there are millions of patients in the system Understanding the query performance impact of different representations of clinical data Understanding the performance impact of features such as enabling role-based access control Creating benchmarks for clinical queries Comparing the impact of new hardware and software on clinical systems Comparting graph queries to non-graph options Testing the performance and feasibility of new algorithms such as real-time fraud detection Background \u00b6 Synthea is a tool for generating synthetic healthcare records. The current version generates a patient record and related clinical information such as allergies, claims, diagnosis, prescriptions, vaccinations, etc. You can generate any arbitrary-sized patient population, you are only limited by the disk space on your servers. Typical Usage \u00b6 A typical use is the following command: 1 $ synthea_run -p 10000 Which will generate the thousand sample patient records with the data broken into CSV files: 1 2 3 4 5 6 7 8 9 10 11 $ ls data/*.csv allergies.csv 12126 careplans.csv 20524 claims.csv 7041 conditions.csv 20525 encounters.csv 13190 immunizations.csv 6049 medications.csv 79675 observations.csv 1463 patients.csv 10000 procedures.csv 5380 You can then load this data into your graph database. The default export format is FHIR, but we find that changing the config files to generate CSV files is more efficient. After the files are generated, you can use the tools in this repository to load this data into TigerGraph.","title":"Home"},{"location":"#synthetic-clinical-data-benchmarking","text":"This microsite describes the generation of synthetic clinical data and the loading of this data into a distributed graph database such as TigerGraph for benchmarking purposes. The process of generating large collections of clinical graph data can be used for: Establishing the baseline performance for bringing an entire clinical record for a patent when there are millions of patients in the system Understanding the query performance impact of different representations of clinical data Understanding the performance impact of features such as enabling role-based access control Creating benchmarks for clinical queries Comparing the impact of new hardware and software on clinical systems Comparting graph queries to non-graph options Testing the performance and feasibility of new algorithms such as real-time fraud detection","title":"Synthetic Clinical Data Benchmarking"},{"location":"#background","text":"Synthea is a tool for generating synthetic healthcare records. The current version generates a patient record and related clinical information such as allergies, claims, diagnosis, prescriptions, vaccinations, etc. You can generate any arbitrary-sized patient population, you are only limited by the disk space on your servers.","title":"Background"},{"location":"#typical-usage","text":"A typical use is the following command: 1 $ synthea_run -p 10000 Which will generate the thousand sample patient records with the data broken into CSV files: 1 2 3 4 5 6 7 8 9 10 11 $ ls data/*.csv allergies.csv 12126 careplans.csv 20524 claims.csv 7041 conditions.csv 20525 encounters.csv 13190 immunizations.csv 6049 medications.csv 79675 observations.csv 1463 patients.csv 10000 procedures.csv 5380 You can then load this data into your graph database. The default export format is FHIR, but we find that changing the config files to generate CSV files is more efficient. After the files are generated, you can use the tools in this repository to load this data into TigerGraph.","title":"Typical Usage"},{"location":"background/","text":"Background \u00b6 This document describes the historical background for how we made a decision to open source these benchmarks. 2017: Database Vendor Benchmarking \u00b6 In 2017 Optum was going through a process of benchmarking various graph databases. We found that we needed a consistent way to evaluate the pros and cons of each database software system. We developed a set of benchmarks using the Synthea synthetic clinical data generation system. We developed a simple labeled property graph [model] and then loaded 10 million synthetic patients into this model running on various vendor databases. After the data was loaded we then performed various stress tests on the system to see what the limits of each software system was. 2020\" Hardware Benchmarking \u00b6 In 2020 we also found that these same benchmarks were ideal to compare different hardware systems. We worked with both TigerGraph and Dell's Healthcare division to create a state-of-the-art hardware system that is custom tuned to perform large graph queries that are distributed over a 8-node cluster. This paper: Dell TigerGraph Case Study is a result of this analysis. Note that this paper found that we could retrieve a full patient record from a group of 11M patients in an incredible 8.5 milliseconds. This was roughly 10x faster than many of our production systems at the time. This incredible result could not have been done without the careful configuration and tuning of the 128-core EPYC hardware and the dedication of the engineers from both TigerGraph and Dell working together over a period of six months. 2022: Optum Open Sources Benchmarks \u00b6 Now in 2022, with the partnership of the Optum Office of Open Source , we are making these benchmarks available to the public using a liberal open source license. This means that any software vendor can also use these benchmarks to see how their systems hold up under the stress of real-world clinical analytics workloads. We hope everyone finds these benchmarks useful at helping us all lower the cost of healthcare for everyone.","title":"Background"},{"location":"background/#background","text":"This document describes the historical background for how we made a decision to open source these benchmarks.","title":"Background"},{"location":"background/#2017-database-vendor-benchmarking","text":"In 2017 Optum was going through a process of benchmarking various graph databases. We found that we needed a consistent way to evaluate the pros and cons of each database software system. We developed a set of benchmarks using the Synthea synthetic clinical data generation system. We developed a simple labeled property graph [model] and then loaded 10 million synthetic patients into this model running on various vendor databases. After the data was loaded we then performed various stress tests on the system to see what the limits of each software system was.","title":"2017: Database Vendor Benchmarking"},{"location":"background/#2020-hardware-benchmarking","text":"In 2020 we also found that these same benchmarks were ideal to compare different hardware systems. We worked with both TigerGraph and Dell's Healthcare division to create a state-of-the-art hardware system that is custom tuned to perform large graph queries that are distributed over a 8-node cluster. This paper: Dell TigerGraph Case Study is a result of this analysis. Note that this paper found that we could retrieve a full patient record from a group of 11M patients in an incredible 8.5 milliseconds. This was roughly 10x faster than many of our production systems at the time. This incredible result could not have been done without the careful configuration and tuning of the 128-core EPYC hardware and the dedication of the engineers from both TigerGraph and Dell working together over a period of six months.","title":"2020\" Hardware Benchmarking"},{"location":"background/#2022-optum-open-sources-benchmarks","text":"Now in 2022, with the partnership of the Optum Office of Open Source , we are making these benchmarks available to the public using a liberal open source license. This means that any software vendor can also use these benchmarks to see how their systems hold up under the stress of real-world clinical analytics workloads. We hope everyone finds these benchmarks useful at helping us all lower the cost of healthcare for everyone.","title":"2022: Optum Open Sources Benchmarks"},{"location":"benchmarks/","text":"Sample Benchmarks \u00b6 Establishing the baseline performance for bringing an entire clinical record for a patent when there are millions of patients in the system Understanding the query performance impact of different representations of clinical data Understanding the performance impact of features such as enabling role-based access control Creating benchmarks for clinical queries Comparing the impact of new hardware and software on clinical systems Comparting graph queries to non-graph options Testing the performance and feasibility of new algorithms such as real-time fraud detection","title":"Benchmarks"},{"location":"benchmarks/#sample-benchmarks","text":"Establishing the baseline performance for bringing an entire clinical record for a patent when there are millions of patients in the system Understanding the query performance impact of different representations of clinical data Understanding the performance impact of features such as enabling role-based access control Creating benchmarks for clinical queries Comparing the impact of new hardware and software on clinical systems Comparting graph queries to non-graph options Testing the performance and feasibility of new algorithms such as real-time fraud detection","title":"Sample Benchmarks"},{"location":"contacts/","text":"Clinical Graph Benchmarking Contacts \u00b6 Optum \u00b6 Dan McCreary Dan.McCreary@gmail.com Dell \u00b6 TBD TigerGraph \u00b6 Kevin Cai kevin.cai@tigergraph.com jon.herke@tigergraph.com jon.herke@tigergraph.com andrew@tigergraph.com andrew@tigergraph.com","title":"Contacts"},{"location":"contacts/#clinical-graph-benchmarking-contacts","text":"","title":"Clinical Graph Benchmarking Contacts"},{"location":"contacts/#optum","text":"Dan McCreary Dan.McCreary@gmail.com","title":"Optum"},{"location":"contacts/#dell","text":"TBD","title":"Dell"},{"location":"contacts/#tigergraph","text":"Kevin Cai kevin.cai@tigergraph.com jon.herke@tigergraph.com jon.herke@tigergraph.com andrew@tigergraph.com andrew@tigergraph.com","title":"TigerGraph"},{"location":"mission/","text":"Open Source Clinical Benchmarking Mission \u00b6 The mission of our team is the following: Provide state-of-the-art clinical benchmarks to help stakeholders fairly understand the tradeoffs of using alternate representations of clinical data. Our values include: Transparency - we value not hiding any of the pros or cons of different architectures Scalability - we focus on not just small clinics and hospitals but analytics of the entire US patient populations Inclusiveness - we value the inputs to our benchmarks regardless of an organization's prior work or the race, gender or background of any individuals that would like to contribute to our site. All are welcome here.","title":"Mission"},{"location":"mission/#open-source-clinical-benchmarking-mission","text":"The mission of our team is the following: Provide state-of-the-art clinical benchmarks to help stakeholders fairly understand the tradeoffs of using alternate representations of clinical data. Our values include: Transparency - we value not hiding any of the pros or cons of different architectures Scalability - we focus on not just small clinics and hospitals but analytics of the entire US patient populations Inclusiveness - we value the inputs to our benchmarks regardless of an organization's prior work or the race, gender or background of any individuals that would like to contribute to our site. All are welcome here.","title":"Open Source Clinical Benchmarking Mission"},{"location":"model/","text":"Clinical Graph Model \u00b6 High Level Model \u00b6","title":"Clinical Graph Model"},{"location":"model/#clinical-graph-model","text":"","title":"Clinical Graph Model"},{"location":"model/#high-level-model","text":"","title":"High Level Model"},{"location":"references/","text":"References \u00b6 Dell-TigerGraph 2020 Clincal Benchmark \u00b6 [Graph Analytics in Healthcare Operations: A case study of TigerGraph on Dell EMC Infrastructure] ( Dell TigerGraph Case Study )","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#dell-tigergraph-2020-clincal-benchmark","text":"[Graph Analytics in Healthcare Operations: A case study of TigerGraph on Dell EMC Infrastructure] ( Dell TigerGraph Case Study )","title":"Dell-TigerGraph 2020 Clincal Benchmark"},{"location":"tigergraph/","text":"TigerGraph Loader \u00b6 Data Model Definition Language (DDL) \u00b6 The model data definition script is here: Load Scripts \u00b6 We provide standard GSQL-shell based loading scripts. These load scripts are here:","title":"TigerGraph Loaders"},{"location":"tigergraph/#tigergraph-loader","text":"","title":"TigerGraph Loader"},{"location":"tigergraph/#data-model-definition-language-ddl","text":"The model data definition script is here:","title":"Data Model Definition Language (DDL)"},{"location":"tigergraph/#load-scripts","text":"We provide standard GSQL-shell based loading scripts. These load scripts are here:","title":"Load Scripts"}]}